{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca504c7",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Frenet Corridor Planner Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd772b8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"./images/img-motivation.png\" alt=\"Motivation\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Motivated by the need for both effectiveness and efficiency, **path-speed decomposition-based trajectory planning** has become a standard approach in autonomous driving. While a global route can be computed offline, real-time generation of adaptive local paths remains critical for safe operation.\n",
    "\n",
    "To address this, we present the **Frenet Corridor Planner (FCP)**—an optimization-based local path planning method designed for smooth and safe navigation around static and dynamic obstacles. In FCP, vehicles are modeled as safety-augmented bounding boxes, and pedestrians are represented using convex hulls, all within the Frenet frame. By determining the appropriate deviation side for each static obstacle, FCP constructs a drivable corridor that respects safety constraints.\n",
    "\n",
    "A modified space-domain bicycle kinematics model is then used to optimize the path, balancing smoothness, boundary clearance, and risk avoidance from dynamic obstacles. The optimized path is subsequently passed to a speed planner to generate the final spatiotemporal trajectory.\n",
    "\n",
    "This tutorial walks through each component of FCP and demonstrates its performance in replanning scenarios.\n",
    "\n",
    "**Full paper**: [FCP on arXiv](https://arxiv.org/abs/2505.03695)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779bab1c",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "* [Example Scenario](#example-scenario)\n",
    "* [Boundary Generation](#boundary-generation)\n",
    "* [Sample FCP Solution](#sample-fcp-solution)\n",
    "* [Dynamic Obstacle Handling](#dynamic-obstacle-handling)\n",
    "* [Replanning with Perception Noise](#replanning-with-perception-noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ead6e",
   "metadata": {},
   "source": [
    "## Example Scenario\n",
    "\n",
    "We begin with a simple corridor-passing scenario involving static obstacles positioned on either side of the path.\n",
    "\n",
    "> **Note:** All coordinates and visualizations are expressed in the Frenet frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c23ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# Ego vehicle parameters\n",
    "ego = [0,0,0] # x, y, yaw\n",
    "ego_l, ego_w = 5, 2 # Length and width of the ego vehicle\n",
    "lon_buffer = 3  # expand bounding box to make the deviated path safer\n",
    "lat_buffer = 1.5 # expand bounding box to make the deviated path safer\n",
    "\n",
    "# Obstacle parameters\n",
    "obs_l, obs_w = 5, 2\n",
    "stat_obss = [] # static obstacles\n",
    "obs1 = [30, 0, np.deg2rad(10), obs_l, obs_w] # x, y, yaw, length, width\n",
    "obs2 = [40, -1, np.deg2rad(7.5), obs_l, obs_w] # x, y, yaw, length, width\n",
    "obs3 = [60, 4, np.deg2rad(-10), obs_l, obs_w] # x, y, yaw, length, width\n",
    "stat_obss.extend([obs1,obs2,obs3])\n",
    "dyn_obss = [] # dynamic obstacles\n",
    "obss = stat_obss + dyn_obss\n",
    "\n",
    "# Add pedestrians\n",
    "N_peds = 30\n",
    "peds = [] # pedestrians\n",
    "random.seed(8)  # For reproducibility\n",
    "for i in range(int(N_peds/2)):\n",
    "    peds.append([random.randint(75,120),random.randint(-2,0)]) \n",
    "    peds.append([random.randint(75,120),random.randint(4,6)]) \n",
    "\n",
    "vis_scenario(ego,[],[],stat_obss,dyn_obss,[],peds=peds,length=ego_l, width=ego_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d650c",
   "metadata": {},
   "source": [
    "## Boundary Generation\n",
    "\n",
    "Boundaries define the lower and upper limits (i.e., **corridor**) for **FCP**, which specifies the valid set of path solutions. Boundary generation involves three key steps:\n",
    "\n",
    "1. **Bounding Box Generation for Static Obstacles**  \n",
    "   For each static obstacle, a bounding box is generated. The robustness of this box depends on the perception system’s accuracy. A practical approach is to use *probabilistic bounding boxes* based on a chosen quantile to account for uncertainty.\n",
    "   \n",
    "   ![Bounding Box](./images/img-bounding-box.png)\n",
    "\n",
    "2. **Classification into Lower and Upper Boundaries**  \n",
    "   Each bounding box is classified as contributing to either the lower or upper boundary, based on estimated gaps relative to the ego vehicle’s position. This step determines which side of the corridor the obstacle constrains. When an obstacle is marginally positioned near the centerline, it can be treated as a risk (soft constraint) in FCP rather than a hard constraint to maintain feasibility and consistency in boundary generation.\n",
    "   \n",
    "   ![Classification](./images/img-bound-classification.png)\n",
    "\n",
    "3. **Boundary Construction**  \n",
    "   Based on the classification, the lower and upper boundaries are constructed. If multiple bounding boxes overlap (especially for pedestrians), a *convex hull* is computed to form a unified boundary. FCP does not require the boundaries to be smooth or linear—any time-efficient method may be used. In this tutorial, we apply a simple *extrema-based* approach that quickly identifies non-smooth boundary shapes.\n",
    "   \n",
    "   ![Construction](./images/img-boundary-construction.png)\n",
    "\n",
    "> **Note:** Boundaries are generated with respect to the **ego vehicle’s center position**. Therefore, bounding boxes must include sufficient buffer space to account for the vehicle’s dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de96846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate boundaries (lb, ub) for the static obstacles\n",
    "# The boundaries are generated based on the bounding boxes of the static obstacles.\n",
    "# The bounding boxes are generated with a buffer to ensure safety.\n",
    "# The bounding boxes are then classified into lower and upper boundaries based on the gap.\n",
    "# TODO:: Update the function to cluser the pedestrians to a convex hull\n",
    "lb,ub,bboxs = update_bounds(ego[0], stat_obss, peds=peds)\n",
    "\n",
    "# Visualize the boundaries\n",
    "vis_scenario(ego,lb,ub,stat_obss,dyn_obss,bboxs, peds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91f9eb",
   "metadata": {},
   "source": [
    "## Sample FCP Solution\n",
    "\n",
    "Given the previously defined lower (red) and upper (blue) boundaries, FCP solves a nonlinear optimization problem using a bicycle kinematics model with a fixed longitudinal step size.\n",
    "\n",
    "The optimization problem is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\min_{u}\\;\\; &d^\\top Q_d d + u^\\top Q_u u + \\lambda_\\text{curve} \\sum_k \\tan^2 u_k\\ \\nonumber\\\\ \n",
    "    &+ \\lambda_\\text{risk} \\sum_k \\left(d_k - \\frac{d^\\text{lb}_{t_k} + d^\\text{ub}_{t_k}}{2} \\right)^2 \n",
    "    + \\lambda_\\text{dyn} \\sum_{i=1}^{N_t^{\\text{dyn}}} \\sum_k \\frac{1}{\\left(\\hat{d}^{(i)}_k - d_k\\right)^2}, \\\\\n",
    "    \\text{subject to:} \\nonumber \\\\\n",
    "    s_{k+1} &= s_k + \\Delta s, \\\\\n",
    "    d_{k+1} &= d_k + \\tan(\\phi_k + u_k)\\, \\Delta s, \\\\\n",
    "    \\phi_{k+1} &= \\phi_k + \\frac{\\Delta s}{\\ell_r} \\cdot \\frac{\\sin u_k}{\\cos(\\phi_k + u_k)}, \\\\\n",
    "    d^\\text{lb}_{t_k} - \\alpha_k &\\leq d_k \\leq d^\\text{ub}_{t_k} + \\alpha_k, \\\\\n",
    "    s_0 &= \\hat{s}_t,\\quad d_0 = \\hat{d}_t,\\quad \\phi_0 = \\hat{\\phi}_t, \\\\\n",
    "    u_k^\\text{min} &\\leq u_k \\leq u_k^\\text{max}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This formulation accounts for:\n",
    "\n",
    "- **Smoothness and control effort** via quadratic penalties on $d$ and $u$\n",
    "- **Path curvature** using a penalty on $\\tan^2 u_k$\n",
    "- **Obstacle clearance risk** by encouraging proximity to the centerline between the lower and upper boundaries\n",
    "- **Dynamic obstacle avoidance** through inverse distance penalties\n",
    "\n",
    "> **Note:** Please refer to the [full paper](https://arxiv.org/abs/2505.03695) for detailed derivation and implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad02a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve FCP\n",
    "sol = fcp_simple(ego, lb[:,1], ub[:,1], S=100, ds=1)\n",
    "\n",
    "vis_scenario(ego,lb,ub,stat_obss,[],bboxs,peds=peds)\n",
    "plt.plot(sol[0],sol[1],label='FCP',linewidth=65, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c72c59",
   "metadata": {},
   "source": [
    "## Dynamic Obstacle Handling\n",
    "\n",
    "Treating dynamic obstacles as static (i.e., imposing hard constraints) is generally unsuitable in FCP, as it can lead to infeasible solutions—especially when dynamic obstacles are nearby. Instead, FCP incorporates dynamic obstacles as **soft constraints** by adding a **risk penalty** term to the objective function.\n",
    "\n",
    "It’s important to note that FCP operates purely in the **spatial (path) domain**—there is no explicit representation of time. However, dynamic obstacles inherently involve temporal behavior. To bridge this gap, we approximate temporal influence by **aggregating the predicted positions** of each dynamic obstacle over a fixed number of future steps ($N$) within the planning horizon.\n",
    "\n",
    "This approach enables time-aware risk avoidance without requiring full spatiotemporal optimization.\n",
    "\n",
    "> FCP relies on a downstream speed planner to appropriately reduce speed and prevent collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add static and dynamic obstacles\n",
    "obss = [[21, -0.5, np.deg2rad(-2), 0, 5, 2], # x, y, yaw, speed, length, width\n",
    "        [71, -0.5, np.deg2rad(2), 0, 5, 2], # x, y, yaw, speed, length, width\n",
    "        [81, 3.5, np.deg2rad(180), 5, 5, 2]] # x, y, yaw, speed, length, width\n",
    "stat_obss = [obs for obs in obss if obs[3] == 0] # static obstacles\n",
    "dyn_obss = [obs for obs in obss if obs[3] != 0] # dynamic obstacles\n",
    "lb,ub,bboxs = update_bounds(ego[0], stat_obss)\n",
    "\n",
    "# Dynamic obstacles are added to the scenario with a prediction\n",
    "# Here, we assume a simple linear prediction for dynamic obstacles -- this can be replaced with a more sophisticated prediction model.\n",
    "# Generate predictions for dynamic obstacles over N time steps\n",
    "N_pred = 10 # prediction time steps\n",
    "dyn_preds = []\n",
    "for obs in dyn_obss:\n",
    "    preds = np.array([[obs[0] + obs[3]*k*1*np.cos(obs[2]),obs[1]+obs[3]*k*1*np.sin(obs[2]),obs[2],obs[3]] for k in range(N_pred)]) # over N time steps\n",
    "    dyn_preds.append(preds)\n",
    "\n",
    "# Solve FCP\n",
    "sol = fcp_simple([0,0,0], lb[:,1], ub[:,1], S=100,dyn_preds=dyn_preds)\n",
    "\n",
    "vis_scenario(ego,lb,ub,stat_obss,dyn_obss,bboxs)\n",
    "plt.plot(sol[0],sol[1],label='FCP',linewidth=65, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144b489",
   "metadata": {},
   "source": [
    "From the above example, the ego vehicle tries to be distant to the expected path of the dynamic obstacle, as we should expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc8311",
   "metadata": {},
   "source": [
    "## Replanning with Perception Noise\n",
    "\n",
    "In real-world scenarios, various sources of noise—such as perception inaccuracies and the unpredictable behavior of other traffic agents—can affect planning. This example demonstrates FCP's ability to produce smooth and consistent behavior despite **perception noise**, which is visualized as jittery boundaries.\n",
    "\n",
    "Replanning results, including images and a video, are saved by default to `./fcp-sim/`.\n",
    "\n",
    "> **Note:** This simulation does not include ego speed planning; the ego vehicle moves at a fixed speed of 1 m/s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import os, random\n",
    "\n",
    "# ====================================\n",
    "# Scenario\n",
    "# ====================================\n",
    "ego_state = [0,0,0] # x, y, yaw\n",
    "obss = [\n",
    "        [20, -2, np.deg2rad(-2), 0, 5, 2], # x, y, yaw, speed, length, width\n",
    "        [30, -0.5, np.deg2rad(2), 0, 5, 2], # x, y, yaw, speed, length, width\n",
    "        [70, -0.5, np.deg2rad(180), 0, 5, 2], # x, y, yaw, speed, length, width\n",
    "        [100, 3.5, np.deg2rad(180), 1.2, 5, 2], # x, y, yaw, speed, length, width\n",
    "    ] \n",
    "\n",
    "# ====================================\n",
    "# Control params\n",
    "# ====================================\n",
    "S = 50 # planning horizion [m]\n",
    "ds = 1 # step size [m]\n",
    "prev_path = False\n",
    "perception_noise = True\n",
    "smooth_out = False # Moving average smoothing\n",
    "sim_N = 50 # Length of simulation\n",
    "sim_dt = 1 # Simulation time step [s]\n",
    "wb_buffer = 0.5 # Buffer for the which_bound check\n",
    "save_figs = True # Save figures\n",
    "N_pred = 10 # Prediction time steps for dynamic obstacles\n",
    "occlusions = False # Consider occlusions in the scenario -- NOTE:: Setting to True will significantly increase the computation time\n",
    "\n",
    "# ====================================\n",
    "# Default params \n",
    "# ====================================\n",
    "ref_lb = -2 # reference path lb\n",
    "ref_ub = 5 # reference path ub\n",
    "lane_lb = -2 # current lb\n",
    "lane_ub = 2 # current ub\n",
    "lane_min_val = -2 # hard limit for the drivable space\n",
    "lane_max_val = 5 # hard limit for the drivable space\n",
    "\n",
    "# ====================================\n",
    "# Initialize\n",
    "# ====================================\n",
    "prevs = []\n",
    "prev_sol = False\n",
    "stat_obss = [obs for obs in obss if obs[3] == 0]\n",
    "dyn_obss = [obs for obs in obss if obs[3] != 0]\n",
    "stat_obss_raw = copy.deepcopy(stat_obss)\n",
    "dyn_obss_raw = copy.deepcopy(dyn_obss)\n",
    "comp_time = []\n",
    "if save_figs:\n",
    "    if sim_N != 0:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        folder_name = \"./fcp-sim/{}\".format(now)\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "# ====================================\n",
    "# Replanning\n",
    "# ====================================\n",
    "for k in range(sim_N): \n",
    "    if occlusions:\n",
    "        stat_obss = get_visible_obss(ego_state, stat_obss)\n",
    "    \n",
    "    # Update bounds\n",
    "    (lb,ub,bboxs,lane_lb,lane_ub) = adaptive_bounds(ego_state, stat_obss, \n",
    "                                  lane_lb=lane_lb, lane_ub=lane_ub, ref_ub=ref_ub,\n",
    "                                  lane_min_val = lane_min_val, lane_max_val = lane_max_val, S=S, ds = ds,\n",
    "                                  evaluate=True, wb_buffer=wb_buffer)\n",
    "    \n",
    "    # Get prediction on others\n",
    "    # NOTE:: add points to reflect the shape of obstacles that are considered as risks\n",
    "    dyn_preds = []\n",
    "    for obs in dyn_obss:\n",
    "        # Prediction here\n",
    "        preds = np.array([[obs[0] + obs[3]*k*1*np.cos(obs[2]) + 0.5*(np.random.uniform()-0.5) * perception_noise, \n",
    "                           obs[1] + obs[3]*k*1*np.sin(obs[2]) + 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[2] + 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[3],\n",
    "                           ] for k in range(N_pred)]\n",
    "                        +[[obs[0]-obs[-2]/2*np.cos(obs[2])+ 0.5*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[1]-obs[-1]/2*np.sin(obs[2]) + 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[2]                        + 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[3]],\n",
    "                        [obs[0]+obs[-2]/2*np.cos(obs[2])+ 0.5*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[1]+obs[-1]/2*np.sin(obs[2])+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[2]+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[3]]]\n",
    "                        +[[obs[0]-obs[-2]*np.cos(obs[2])+ 0.5*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[1]-obs[-1]/2*np.sin(obs[2])+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[2]+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                           obs[3]],\n",
    "                        [obs[0]+obs[-2]*np.cos(obs[2])+ 0.5*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[1]+obs[-1]/2*np.sin(obs[2])+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[2]+ 0.2*(np.random.uniform()-0.5) * perception_noise,\n",
    "                         obs[3]]]\n",
    "                           ) \n",
    "        dyn_preds.append(preds)\n",
    "\n",
    "    if prev_path:\n",
    "        # NOTE:: the prev_path is a list of two lists, where the first list is x and the second list is y\n",
    "        # NOTE:: it is assumed that ego is at the first point of the prev_path for simplicity.\n",
    "        prev_sol = [prev_path[0][1:],prev_path[1][1:]]\n",
    "\n",
    "    # Run FCP\n",
    "    sol, elapsed = fcp_simple(ego_state, lb[:,1], ub[:,1], S=S, ds=ds, dyn_preds=dyn_preds,  prev_sol = prev_sol, return_time = True); \n",
    "    comp_time.append(elapsed)\n",
    "    \n",
    "    # Smoothing filter\n",
    "    if smooth_out and len(prevs)>0:\n",
    "        prev_path = smooth_path(prevs,sol,3)\n",
    "    else:\n",
    "        prev_path = sol\n",
    "    prevs.append(prev_path)\n",
    "    if len(prevs) > 5:\n",
    "        prevs.pop(0)\n",
    "\n",
    "    # Visualize the solution\n",
    "    if occlusions:\n",
    "        vis_occlusions(ego_state,lb,ub,stat_obss_raw, stat_obss, bboxs, dyn_preds=dyn_preds)\n",
    "    else:\n",
    "        vis_scenario(ego_state,lb,ub,stat_obss,dyn_obss,bboxs, dyn_preds =dyn_preds)\n",
    "    plt.plot(prev_path[0],prev_path[1],label='FCP',linewidth=65, alpha=0.5)\n",
    "    plt.text(0,5.5, \"Avg comp. time {} max {}\".format(round(np.mean(comp_time)*1000)/1000,round(max(comp_time)*1000)/1000), fontsize=20)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plots for visualization\n",
    "    fn = \"{}/{}.png\".format(folder_name,k)\n",
    "    with open(fn,\"w\") as f:\n",
    "        plt.savefig(fn)\n",
    "    plt.close() # clear the figure for the next iteration\n",
    "\n",
    "    # Simulate EGO\n",
    "    lookahead = 4\n",
    "    vec1 = [np.cos(ego_state[2]),np.sin(ego_state[2])]\n",
    "    vec2 = [prev_path[0][lookahead]-ego_state[0],prev_path[1][lookahead]-ego_state[1]]\n",
    "    delta_heading = np.arccos(np.dot(vec1,vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "    arctan_heading = np.arctan(vec2[1]/vec2[0])\n",
    "    ego_state[0] = np.interp(sim_dt,[0,ds],[prev_path[0][0],prev_path[0][1]]) \n",
    "    ego_state[1] = np.interp(sim_dt,[0,ds],[prev_path[1][0],prev_path[1][1]]) \n",
    "    ego_state[2] = arctan_heading\n",
    "\n",
    "    # Simulate TRAFFIC\n",
    "    random.seed(7)\n",
    "    for obs in dyn_obss:\n",
    "        # Simulate here\n",
    "        obs[0] += obs[3]*sim_dt*np.cos(obs[2])\n",
    "        obs[1] += obs[3]*sim_dt*np.sin(obs[2])\n",
    "        obs[2] = obs[2] \n",
    "        obs[3] = obs[3] \n",
    "\n",
    "    if perception_noise:\n",
    "        # Add noise to static obstacles\n",
    "        for (obs,raw) in zip(stat_obss, stat_obss_raw):\n",
    "            obs[0] = raw[0] + 0.5*(np.random.uniform()-0.5)\n",
    "            obs[1] = raw[1] + 0.2*(np.random.uniform()-0.5)\n",
    "            obs[2] = raw[2] + 0.2*(np.random.uniform()-0.5)\n",
    "\n",
    "# Merge pngs to mp4 \n",
    "if sim_N != 0:\n",
    "    os.system(\"ffmpeg -framerate 5 -i '{}/%d.png' -c:v libx264 -r 30 -pix_fmt yuv420p '{}/out.mp4' -loglevel quiet\".format(folder_name,folder_name))\n",
    "    print(\"Mean comp time {} max {} min {} std {}\".format(np.mean(comp_time),max(comp_time),min(comp_time),np.std(comp_time)))\n",
    "    print(\"Video saved to {}/out.mp4\".format(folder_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
